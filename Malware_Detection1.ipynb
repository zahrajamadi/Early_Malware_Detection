{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k66RFRHBjiVi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_47024\\4229580949.py:2: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import BaggingClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyDxt_Nbjol6",
        "outputId": "bf516f89-b247-41a5-e9fb-753f636a861c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43876, 102)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"dynamic_api_call_sequence_per_malware_100_0_306.csv\" )\n",
        "\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create data samples and they corresponding label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci6mYBApXEzB",
        "outputId": "044b9fe4-fd50-4f9c-c2ec-23f832cd2bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43875, 100)\n",
            "(43875,)\n"
          ]
        }
      ],
      "source": [
        "X = df.iloc[1:, 1:-1].values\n",
        "y = df.iloc[1:, -1].values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the strings of 2 and 3 consecutive API calls as new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfx0xJOEkA38",
        "outputId": "43b7d4ec-f392-457f-9455-127098497620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(43875, 34353)\n",
            "(43875,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "api_calls = X\n",
        "api_calls_str = [' '.join(map(str, sequence)) for sequence in api_calls]\n",
        "\n",
        "# Create a CountVectorizer object with the desired n-gram range\n",
        "ngram_range = (2, 3)  # use n-grams of length 2 and 3\n",
        "vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
        "\n",
        "# Fit the vectorizer to the API call sequences and transform them to n-gram vectors\n",
        "X = vectorizer.fit_transform(api_calls_str)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "# Print the vocabulary of the vectorizer (i.e., the set of unique n-grams)\n",
        "#print(vectorizer.vocabulary_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a single XGBoost classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sBFMAsn-JyMc"
      },
      "outputs": [],
      "source": [
        "clf = XGBClassifier(learning_rate= 0.1, max_depth = 5, n_estimators = 300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UH5z3F3nDgz",
        "outputId": "1b503741-97a1-4884-9138-db2a25bd868d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(68480,)\n",
            "34240\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state=42)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled_train, y_resampled_train = ros.fit_resample(X_train, y_train) #Oversampling the minority class\n",
        "X_resampled_test, y_resampled_test = ros.fit_resample(X_test, y_test)\n",
        "minority_class_size = np.count_nonzero(y_resampled_train == 0)\n",
        "\n",
        "\n",
        "print(y_resampled_train.shape)\n",
        "print(minority_class_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training the model and make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gcVLKXTUYEH",
        "outputId": "fae8f8db-662c-4d05-cf08-9697b4df98c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "clf.fit(X_resampled_train, y_resampled_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = clf.predict(X_resampled_test)\n",
        "print(y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature importance analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "F_sZp9DTJ8i6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get the feature importance scores from the XGBoost classifier\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "# Get the feature importance scores from the XGBoost classifier\n",
        "feat_importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importance scores and feature names\n",
        "feat_df = pd.DataFrame({'feature': feature_names, 'importance': feat_importances})\n",
        "\n",
        "# Sort the DataFrame by importance score\n",
        "feat_df = feat_df.sort_values('importance', ascending=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Printing the top important features and their ocurrences among goodware and malware samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiaZSdTjfnq8",
        "outputId": "a94b7882-3d1b-45a1-9e11-bcf261f137dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240 117 82: class 0 - 1779, class 1 - 12287\n",
            "117 297 199: class 0 - 0, class 1 - 6345\n",
            "35 208 240: class 0 - 625, class 1 - 14365\n",
            "199 264: class 0 - 17349, class 1 - 16475\n",
            "215 37: class 0 - 549, class 1 - 5129\n",
            "114 215: class 0 - 14582, class 1 - 3064\n",
            "117 215 260: class 0 - 12165, class 1 - 934\n",
            "215 37 158: class 0 - 397, class 1 - 4993\n",
            "202 260: class 0 - 13248, class 1 - 8108\n",
            "117 215 89: class 0 - 44, class 1 - 2276\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Extract the top 10 important feature names\n",
        "top_features = feat_df.head(10)['feature'].values.tolist()\n",
        "\n",
        "# Step 2: Get the index positions of these feature names in the `vectorizer.vocabulary_` dictionary\n",
        "top_feature_indices = [vectorizer.vocabulary_[feature] for feature in top_features]\n",
        "\n",
        "# Step 3: Create a new dataframe `X_top_features` by selecting only the columns corresponding to the top 10 features\n",
        "X_top_features = X_resampled_train[:, top_feature_indices]\n",
        "\n",
        "# Step 4: Calculate the sum of each column in `X_top_features` to get the total occurrence count for each of the top 10 features\n",
        "class_0_sum = X_top_features[y_resampled_train == 0].sum(axis=0)\n",
        "class_1_sum = X_top_features[y_resampled_train == 1].sum(axis=0)\n",
        "\n",
        "# Step 5: Print out the total occurrence counts for each of the top 10 features in order to determine which class they mainly belong to\n",
        "for i, feature in enumerate(top_features):\n",
        "    print(f\"{feature}: class 0 - {class_0_sum[0, i]}, class 1 - {class_1_sum[0, i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PCy_VWvSJU3",
        "outputId": "e52147e5-32da-4b0d-f797-ff08c483aa00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "7867\n",
            "0.955411407199626\n",
            "ROC AUC score: 0.955\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(type(y_pred))\n",
        "num_zeros = np.count_nonzero(y_pred == 0)\n",
        "print(num_zeros)\n",
        "accuracy = accuracy_score(y_resampled_test, y_pred)\n",
        "print(accuracy)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# Compute the ROC AUC score\n",
        "auc = roc_auc_score(y_resampled_test, y_pred)\n",
        "\n",
        "# Print the ROC AUC score\n",
        "print(f\"ROC AUC score: {auc:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grid search for finding the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfPhss-HAEPZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "    \"learning_rate\": [0.05, 0.01, 0.1],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"n_estimators\": [100, 200, 300]\n",
        "}\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=clf,\n",
        "    param_grid=params,\n",
        "    cv=5, # number of cross-validation folds\n",
        "    verbose=3 \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identifying the best XGBoost classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWQghUCEGdAv",
        "outputId": "f02329ba-996a-42d7-bcde-328006f471c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 1: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300} with score 0.9947721962616821\n",
            "Rank 2: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300} with score 0.9929906542056074\n",
            "Rank 3: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200} with score 0.9926547897196262\n"
          ]
        }
      ],
      "source": [
        "results = grid_search.cv_results_\n",
        "top_3_idx = results['mean_test_score'].argsort()[-3:][::-1]\n",
        "for i, idx in enumerate(top_3_idx):\n",
        "    print(f\"Rank {i+1}: {results['params'][idx]} with score {results['mean_test_score'][idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create XGBoots bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn0BHuNWHmYd",
        "outputId": "c8cd7df9-6ace-4e46-ba57-f8cd584481b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define the hyperparameters for each XGBoost classifier\n",
        "params1 = {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1}\n",
        "params2 = {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}\n",
        "params3 = {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.01}\n",
        "\n",
        "# Create an instance of the XGBoost classifier with each set of hyperparameters\n",
        "xgb_clf1 = XGBClassifier(**params1)\n",
        "xgb_clf2 = XGBClassifier(**params2)\n",
        "xgb_clf3 = XGBClassifier(**params3)\n",
        "\n",
        "# Create an instance of the BaggingClassifier and pass the XGBoost classifiers to it\n",
        "bagging_clf = BaggingClassifier(base_estimator=xgb_clf1, n_estimators=3, random_state=42)\n",
        "\n",
        "# Train the BaggingClassifier on the training set\n",
        "bagging_clf.fit(X_resampled_train, y_resampled_train)\n",
        "\n",
        "# Make predictions on the test set using the BaggingClassifier\n",
        "y_pred = bagging_clf.predict(X_resampled_test)\n",
        "\n",
        "# Calculate the accuracy of the BaggingClassifier\n",
        "accuracy = accuracy_score(y_resampled_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ_5GfhoCr3H",
        "outputId": "e68a30c0-9a08-44f6-9b6f-6bb1f19b4632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9599111734455353\n"
          ]
        }
      ],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jn1CxT5IQ3I",
        "outputId": "8c152ee7-7055-4935-f009-611f13fd778a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.9942    0.9252    0.9585      8556\n",
            "     class 1     0.9301    0.9946    0.9613      8556\n",
            "\n",
            "    accuracy                         0.9599     17112\n",
            "   macro avg     0.9621    0.9599    0.9599     17112\n",
            "weighted avg     0.9621    0.9599    0.9599     17112\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming y_true and y_pred are the true and predicted labels, respectively\n",
        "target_names = ['class 0', 'class 1']  # List of class names\n",
        "\n",
        "# Compute classification report\n",
        "report = classification_report(y_resampled_test, y_pred, target_names=target_names, digits=4)\n",
        "\n",
        "# Print the report\n",
        "print(report)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
